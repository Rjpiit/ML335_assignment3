{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rjpiit/ML335_assignment3/blob/main/init_Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZMdRtQsiP78",
        "outputId": "2bf26d34-1a4c-4d0e-c74b-fe8ad6d7a37c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/700], Loss: 6.1568\n",
            "Epoch [100/700], Loss: 7.0942\n",
            "Epoch [150/700], Loss: 5.9727\n",
            "Epoch [200/700], Loss: 5.2496\n",
            "Epoch [250/700], Loss: 5.1649\n",
            "Epoch [300/700], Loss: 5.1167\n",
            "Epoch [350/700], Loss: 5.1999\n",
            "Epoch [400/700], Loss: 5.1855\n",
            "Epoch [450/700], Loss: 5.1127\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess text data from a URL\n",
        "def load_text(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an error for bad responses\n",
        "    text = response.text\n",
        "    return text\n",
        "\n",
        "# Preprocess text: clean and split into sentences\n",
        "def preprocess_text(text):\n",
        "    text = re.sub('[^a-zA-Z0-9 \\.]', '', text)  # Keep alphanumeric and periods\n",
        "    sentences = text.lower().split('.')\n",
        "    return [sentence.strip() for sentence in sentences if sentence]\n",
        "\n",
        "# Load text data from the specified URL\n",
        "url = 'http://prize.hutter1.net/'  # Replace with the desired URL\n",
        "text_data = load_text(url)\n",
        "processed_sentences = preprocess_text(text_data)\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = set(\" \".join(processed_sentences).split())\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Dataset class for text generation\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, sentences, word_to_idx, seq_length=5):\n",
        "        self.data = []\n",
        "        self.seq_length = seq_length\n",
        "        self.word_to_idx = word_to_idx\n",
        "        for sentence in sentences:\n",
        "            words = sentence.split()\n",
        "            if len(words) >= seq_length:\n",
        "                for i in range(len(words) - seq_length):\n",
        "                    seq = words[i:i+seq_length]\n",
        "                    target = words[i + seq_length]\n",
        "                    self.data.append((seq, target))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, target = self.data[idx]\n",
        "        seq_indices = torch.tensor([self.word_to_idx[word] for word in seq], dtype=torch.long)\n",
        "        target_index = torch.tensor(self.word_to_idx[target], dtype=torch.long)\n",
        "        return seq_indices, target_index\n",
        "\n",
        "# Define the MLP model\n",
        "class MLPTextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=1024, seq_length=5):\n",
        "        super(MLPTextGenerator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.fc1 = nn.Linear(seq_length * embedding_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Set parameters and load dataset\n",
        "seq_length = 5\n",
        "embedding_dim = 64\n",
        "hidden_dim = 1024\n",
        "batch_size = 16\n",
        "epochs = 700  # Adjust epochs based on the dataset size\n",
        "\n",
        "dataset = TextDataset(processed_sentences, word_to_idx, seq_length=seq_length)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "model = MLPTextGenerator(vocab_size, embedding_dim, hidden_dim, seq_length)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        inputs, targets = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if (epoch + 1) % 50 == 0:  # Print loss every 50 epochs\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Extract embeddings for visualization\n",
        "embeddings = model.embedding.weight.detach().numpy()\n",
        "\n",
        "# t-SNE visualization\n",
        "def visualize_embeddings(embeddings, vocab, highlighted_words=None):\n",
        "    tsne = TSNE(n_components=2, random_state=0, perplexity=5)\n",
        "    embeddings_2d = tsne.fit_transform(embeddings)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    for i, label in enumerate(vocab):\n",
        "        x, y = embeddings_2d[i]\n",
        "        plt.scatter(x, y)\n",
        "        if highlighted_words and label in highlighted_words:\n",
        "            plt.annotate(label, (x, y), textcoords=\"offset points\", xytext=(0, 5), ha='center', color='red')\n",
        "        else:\n",
        "            plt.annotate(label, (x, y), textcoords=\"offset points\", xytext=(0, 5), ha='center')\n",
        "    plt.title(\"Word Embeddings t-SNE Visualization\")\n",
        "    plt.xlabel(\"t-SNE Dimension 1\")\n",
        "    plt.ylabel(\"t-SNE Dimension 2\")\n",
        "    plt.show()\n",
        "\n",
        "highlighted_words = [\"startup\", \"technology\", \"founder\", \"funding\"]  # Example words related to Paul Graham's themes\n",
        "visualize_embeddings(embeddings, word_to_idx.keys(), highlighted_words)\n",
        "\n",
        "# Text-based word generation function\n",
        "def generate_text(seed_text, num_words=10):\n",
        "    words = seed_text.lower().split()\n",
        "    if len(words) < seq_length:\n",
        "        print(f\"Seed text must contain at least {seq_length} words.\")\n",
        "        return None\n",
        "\n",
        "    generated_text = words[:]\n",
        "    for _ in range(num_words):\n",
        "        seed_tokens = [word_to_idx.get(word, None) for word in generated_text[-seq_length:]]\n",
        "        if None in seed_tokens:\n",
        "            print(\"Some words in the seed text are not in the vocabulary.\")\n",
        "            return \" \".join(generated_text)\n",
        "\n",
        "        seed_tensor = torch.tensor(seed_tokens, dtype=torch.long).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            predictions = model(seed_tensor)\n",
        "            predicted_idx = torch.argmax(predictions, dim=1).item()\n",
        "            predicted_word = idx_to_word[predicted_idx]\n",
        "\n",
        "        generated_text.append(predicted_word)\n",
        "\n",
        "    return \" \".join(generated_text)\n",
        "\n",
        "# Generate text based on a seed sentence\n",
        "seed_text = \"Being able to \"\n",
        "generated_text = generate_text(seed_text, num_words=20)\n",
        "print(\"Generated text:\", generated_text)\n"
      ]
    }
  ]
}